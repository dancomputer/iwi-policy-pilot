
"""
make_payouts_from_prediction_2022_assignments.py

Inputs:
  - predicted_2022.csv: columns [pixel, prediction] (or [Index_ID, Prediction])
  - NKASI - MAIZE - revised 2025.xlsx: contains per-farmer rows with SN, Latitude, Longitude, Loan Amount
  - metadata CSV (Optimistic-metadata): contains [pixel, Lat, Lon]
  - MakeExogenousExcelInputDataframe.py must be importable to get:
      * Threshold_Yield, Attach, Detach per Index_ID
      * Pixel_ID string
      * Pixel_Loan_Amount (aggregated at pixel)
      * Sum_Insured (if not present, we derive: 0.4 * Pixel_Loan_Amount)

Outputs:
  1) payouts_per_pixel_2022.csv  with columns: ['Pixel ID', 'Modelled Yield', 'Payout%', 'Payout']
  2) payouts_per_farmer_2022.csv with columns: ['Pixel ID', 'Farmer SN', 'Payout']

Function:
  make_payout_dfs(predicted_csv_path, nkasi_xlsx, metadata_csv, save_csv=True)
    -> (df_pixel, df_farmer)

Author: generated by ChatGPT
"""
from pathlib import Path
from __future__ import annotations
import argparse
from typing import Tuple, Optional, List

import numpy as np
import pandas as pd

from MakeExogenousExcelInputDataframe import load_and_merge  # type: ignore


def _pick_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    lower_map = {c.lower(): c for c in df.columns}
    for cand in candidates:
        if cand.lower() in lower_map:
            return lower_map[cand.lower()]
    return None


def _ensure_index_id(df: pd.DataFrame) -> pd.DataFrame:
    """Normalize any 'pixel' column in predictions to 'Index_ID' (int)."""
    df = df.copy()
    col = _pick_col(df, ["Index_ID", "Pixel", "pixel", "index_id"])
    if not col:
        raise ValueError("Prediction CSV must contain a 'pixel' (or 'Pixel'/'Index_ID') column.")
    df = df.rename(columns={col: "Index_ID"})
    df["Index_ID"] = pd.to_numeric(df["Index_ID"], errors="coerce").astype("Int64")
    return df


def _ensure_prediction(df: pd.DataFrame) -> pd.DataFrame:
    """Normalize 'prediction' column to 'Prediction' (float)."""
    df = df.copy()
    col = _pick_col(df, ["Prediction", "prediction", "Yield", "yield"])
    if not col:
        raise ValueError("Prediction CSV must contain a 'prediction' column.")
    df = df.rename(columns={col: "Prediction"})
    df["Prediction"] = pd.to_numeric(df["Prediction"], errors="coerce")
    return df


def _haversine_meters(lat1, lon1, lat2, lon2):
    # vectorized haversine distance in meters
    R = 6371000.0
    phi1 = np.radians(lat1)
    phi2 = np.radians(lat2)
    dphi = phi2 - phi1
    dlambda = np.radians(lon2 - lon1)
    a = np.sin(dphi/2.0)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dlambda/2.0)**2
    return 2 * R * np.arcsin(np.sqrt(a))


def build_farmer_assignments(nkasi_xlsx: str, metadata_csv: str) -> pd.DataFrame:
    """
    Return per-farmer assignments with columns:
      ['Index_ID', 'Farmer SN', 'Loan_Amount']

    Map each farmer (NKASI row) to nearest pixel in metadata (by Lat/Lon).
    """
    nk = pd.read_excel(nkasi_xlsx, sheet_name=0)

    # Identify columns in NKASI workbook
    farmer_col = _pick_col(nk, ["Farmer SN", "SN", "FarmerID", "Farmer Id"])
    lat_col = _pick_col(nk, ["Latitude", "lat", "LAT"])
    lon_col = _pick_col(nk, ["Longitude", "lon", "LONGITUDE", "Lng"])
    loan_col = _pick_col(nk, ["Loan Amount", "Loan_Amount", "LoanAmount"])
    if not all([farmer_col, lat_col, lon_col, loan_col]):
        raise KeyError("NKASI workbook must contain farmer id (SN), Latitude, Longitude, and Loan Amount columns.")

    sub = nk[[farmer_col, lat_col, lon_col, loan_col]].copy()
    sub = sub.rename(columns={farmer_col: "Farmer SN", lat_col: "Latitude", lon_col: "Longitude", loan_col: "Loan_Amount"})
    sub["Loan_Amount"] = pd.to_numeric(sub["Loan_Amount"], errors="coerce").fillna(0.0)
    sub = sub.dropna(subset=["Latitude", "Longitude"]).reset_index(drop=True)

    # Metadata pixels (with Lat/Lon)
    meta = pd.read_csv(metadata_csv)
    pcol = _pick_col(meta, ["pixel", "Pixel", "Index_ID", "index_id"])
    mlat = _pick_col(meta, ["Lat", "Latitude", "lat"])
    mlon = _pick_col(meta, ["Lon", "Longitude", "lon"])
    if not all([pcol, mlat, mlon]):
        raise KeyError("Metadata CSV must contain pixel id and coordinates (pixel, Lat, Lon).")
    meta = meta[[pcol, mlat, mlon]].rename(columns={pcol: "Index_ID", mlat: "Lat", mlon: "Lon"}).dropna()
    meta["Index_ID"] = pd.to_numeric(meta["Index_ID"], errors="coerce").astype("Int64")

    # Brute-force nearest pixel (small enough to be fine; switch to KDTree for huge data)
    farmer_lat = sub["Latitude"].to_numpy(dtype=float)
    farmer_lon = sub["Longitude"].to_numpy(dtype=float)
    pix_lat = meta["Lat"].to_numpy(dtype=float)
    pix_lon = meta["Lon"].to_numpy(dtype=float)

    nearest_pixels = []
    for la, lo in zip(farmer_lat, farmer_lon):
        dists = _haversine_meters(la, lo, pix_lat, pix_lon)
        idx = int(np.argmin(dists))
        nearest_pixels.append(meta["Index_ID"].iloc[idx])

    assign = pd.DataFrame({
        "Index_ID": nearest_pixels,
        "Farmer SN": sub["Farmer SN"].tolist(),
        "Loan_Amount": sub["Loan_Amount"].tolist(),
    })
    assign["Index_ID"] = assign["Index_ID"].astype("Int64")
    return assign


def make_payout_dfs(predicted_csv_path: str,
                    nkasi_xlsx: str,
                    metadata_csv: str,
                    save_csv: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Produce the two required DataFrames:
      - df_pixel:  ['Pixel ID', 'Modelled Yield', 'Payout%', 'Payout']
      - df_farmer: ['Pixel ID', 'Farmer SN', 'Payout']
    """
    # 1) Load df_final to get per-pixel meta (Attach/Detach, Threshold_Yield, Pixel_Loan_Amount, Pixel_ID, Sum_Insured)
    df_final = load_and_merge()
    need_cols = ["Index_ID", "Pixel_ID", "Threshold_Yield", "Attach", "Detach", "Pixel_Loan_Amount"]
    missing = [c for c in need_cols if c not in df_final.columns]
    if missing:
        raise KeyError(f"df_final missing columns: {missing}")

    meta_cols = need_cols + (["Sum_Insured"] if "Sum_Insured" in df_final.columns else [])
    meta = (
        df_final[meta_cols]
        .dropna(subset=["Index_ID"])
        .drop_duplicates(subset=["Index_ID"])
        .reset_index(drop=True)
    )
    if "Sum_Insured" not in meta.columns or meta["Sum_Insured"].isna().any():
        meta["Sum_Insured"] = 0.4 * meta["Pixel_Loan_Amount"]

    # 2) Read predictions
    pred = pd.read_csv(predicted_csv_path)
    pred = _ensure_index_id(pred)
    pred = _ensure_prediction(pred)

    # 3) Merge predictions with meta and compute pixel payout
    base = pd.merge(pred, meta, on="Index_ID", how="left", validate="m:1").dropna(subset=["Pixel_ID"]).reset_index(drop=True)
    base["Modelled Yield"] = base["Prediction"] * base["Threshold_Yield"]

    def _payout_fraction(row):
        ya, att, det = row["Modelled Yield"], row["Attach"], row["Detach"]
        if pd.isna(ya) or pd.isna(att) or pd.isna(det) or att == det:
            return 0.0
        if ya > att:
            return 0.0
        if ya < det:
            return 1.0
        return float((att - ya) / (att - det))

    base["Payout%"] = base.apply(_payout_fraction, axis=1)
    base["Payout"] = base["Payout%"] * base["Sum_Insured"]

    df_pixel = base[["Pixel_ID", "Modelled Yield", "Payout%", "Payout"]].rename(columns={"Pixel_ID": "Pixel ID"})
    df_pixel = df_pixel.sort_values("Pixel ID").reset_index(drop=True)

    # 4) Build per-farmer assignments (Farmer SN, Loan_Amount, Index_ID)
    assign = build_farmer_assignments(nkasi_xlsx, metadata_csv)
    # 5) Join per-farmer with pixel payout and use Pixel_Loan_Amount from meta
    join = pd.merge(
        assign,
        base[["Index_ID", "Payout", "Pixel_ID"]],
        on="Index_ID", how="inner", validate="m:1"
    )
    join = pd.merge(
        join,
        meta[["Index_ID", "Pixel_Loan_Amount"]],
        on="Index_ID", how="left", validate="m:1"
    )
    # Allocation
    join["Payout"] = join["Payout"] * (join["Loan_Amount"] / join["Pixel_Loan_Amount"].replace({0: np.nan}))
    join["Payout"] = join["Payout"].fillna(0.0)

    df_farmer = join[["Pixel_ID", "Farmer SN", "Payout"]].rename(columns={"Pixel_ID": "Pixel ID"})
    df_farmer = df_farmer.sort_values(["Pixel ID", "Farmer SN"]).reset_index(drop=True)

    # Optional sanity check (not fatal): farmer sums vs pixel payout
    check = df_farmer.groupby("Pixel ID", as_index=False)["Payout"].sum().rename(columns={"Payout": "FarmerSum"})
    comp = pd.merge(check, df_pixel[["Pixel ID", "Payout"]].rename(columns={"Payout": "PixelPayout"}), on="Pixel ID", how="inner")
    comp["diff"] = (comp["FarmerSum"] - comp["PixelPayout"]).abs()
    large = comp[comp["diff"] > 1e-4]
    if not large.empty:
        print("Warning: Some pixel-level farmer sums differ from pixel payout (check Pixel_Loan_Amount consistency).")

    # 6) Save
    if save_csv:
        out_dir = Path(predicted_csv_path).parent
        df_pixel.to_csv(out_dir / "payouts_per_pixel_2022.csv", index=False)
        df_farmer.to_csv(out_dir / "payouts_per_farmer_2022.csv", index=False)
        print(f"Saved: {out_dir / 'payouts_per_pixel_2022.csv'}")
        print(f"Saved: {out_dir / 'payouts_per_farmer_2022.csv'}")

    return df_pixel, df_farmer


def _main():
    ap = argparse.ArgumentParser(description="Compute 2022 payouts and apportion to farmers using NKASI workbook + df_final meta.")
    ap.add_argument("--predicted", required=True, help="Path to predicted_2022.csv (columns: pixel, prediction)")
    ap.add_argument("--nkasi_xlsx", required=True, help="Path to 'NKASI - MAIZE - revised 2025.xlsx'")
    ap.add_argument("--metadata_csv", required=True, help="Path to metadata CSV with columns: pixel, Lat, Lon")
    ap.add_argument("--no-save", action="store_true", help="Do not write CSV outputs to disk")
    args = ap.parse_args()

    make_payout_dfs(
        predicted_csv_path=args.predicted,
        nkasi_xlsx=args.nkasi_xlsx,
        metadata_csv=args.metadata_csv,
        save_csv=not args.no_save,
    )


if __name__ == "__main__":
    #usage python make_payouts_from_prediction_2022_assignments.py ^
  #--predicted "C:\Users\danie\NecessaryM1InternshipCode\ProjectRice\OutputCalendarDays180_PredictionMaize_1982_2022_SPARSE\predicted_2022.csv" ^
  #--nkasi_xlsx "C:\Users\danie\NecessaryM1InternshipCode\ProjectRice\PolicyPilot\iwi-policy-pilot\data\NKASI - MAIZE - revised 2025.xlsx" ^
  #--metadata_csv "C:\Users\danie\NecessaryM1InternshipCode\ProjectRice\OutputCalendarDays180_Maize_1982_2021_SPARSE\ThreeVariableContiguous-SyntheticYield-Optimistic-metadata.csv"
    df_pixel, df_farmer = make_payout_dfs(
        predicted_csv_path=r"C:\Users\danie\NecessaryM1InternshipCode\ProjectRice\OutputCalendarDays180_PredictionMaize_1982_2022_SPARSE\predicted_2022.csv",
        nkasi_xlsx=r"C:\Users\danie\NecessaryM1InternshipCode\ProjectRice\PolicyPilot\iwi-policy-pilot\data\NKASI - MAIZE - revised 2025.xlsx",
        metadata_csv=r"C:\Users\danie\NecessaryM1InternshipCode\ProjectRice\OutputCalendarDays180_Maize_1982_2021_SPARSE\ThreeVariableContiguous-SyntheticYield-Optimistic-metadata.csv")
